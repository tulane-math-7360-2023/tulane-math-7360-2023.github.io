---
title: "Lab 11 Neural Networks"
author: "Dr. Xiang Ji @ Tulane University"
date: "Nov 17, 2023"
output:
  html_document:
    toc: true
    toc_depth: 4  
subtitle: MATH-7360 Data Analysis
csl: ../apa.csl
---

```{r setup, include=FALSE}
rm(list = ls()) # clean-up workspace
knitr::opts_chunk$set(fig.align = 'center', cache = FALSE)
```

```{r}
sessionInfo()
library(tidyverse)
```

We apply neural network for handwritten digit recognition in this lab.

## Data
We use the **MNIST** database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits ($28 \times 28$) that is commonly used for training and testing machine learning algorithms.

- 60,000 training images, 10,000 testing images. 

You can prepare the data by the following code

```{r}
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```
Training set:
```{r}
dim(x_train)
dim(y_train)
```

Let's take a look over the first 10 images in the training set.

```{r}
for (i in 1:10) {
  (image(t(x_train[i, 28:1,]), useRaster=TRUE, axes=FALSE, col=grey(seq(0, 1, length = 256)), main = y_train[i]))
}
```

Vectorize $28 \times 28$ images into $784$-vectors and scale entries to [0, 1]:
```{r}
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
dim(x_train)
dim(x_test)
```

Encode $y$ as binary class matrix:
```{r}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
dim(y_train)
dim(y_test)
head(y_train)
```

## Q1

Fit a multinomial logit regression model to the training set and test the accuracy with the test set.
Plot the first 10 digits in the test set and compare with their predicted value.

```{r}
mlogit <- keras_model_sequential() 
mlogit %>% 
  layer_dense(units = 10, activation = 'softmax', input_shape = c(784))
summary(mlogit)
```

```{r}
# compile model
mlogit %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```
```{r}
# fit model
mlogit_history <- mlogit %>% fit(
  x_train, y_train, 
  epochs = 20, batch_size = 128, 
  validation_split = 0.2
)
```

```{r}
# Evaluate model performance on the test data:
mlogit.score <- mlogit %>% evaluate(x_test, y_test)
```
Generate predictions on new data:
```{r}
y_predict <- mlogit %>% predict(x_test) %>% k_argmax() %>% as.array()
for (i in 1:10) {
  (image(t(mnist$test$x[i, 28:1,]), useRaster=TRUE, axes=FALSE, col=grey(seq(0, 1, length = 256)), main = y_predict[i]))
}
```


## Q2

Fit a multi-layer neural network and perform the task in Q0 again.

You can refer to this example code. <https://tensorflow.rstudio.com/guide/keras/examples/mnist_mlp/>

```{r}
mmlp <- keras_model_sequential()
mmlp %>%
  layer_dense(units = 256,
              activation = 'relu',
              input_shape = c(784)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')

summary(mmlp)

mmlp %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# Training & Evaluation ----------------------------------------------------
batch_size <- 128
num_classes <- 10
epochs <- 30

# Fit model to data
history <- mmlp %>% fit(
  x_train,
  y_train,
  batch_size = batch_size,
  epochs = epochs,
  verbose = 1,
  validation_split = 0.2
)

plot(history)

mmlp.score <- mmlp %>% evaluate(x_test, y_test,
                           verbose = 0)

# Output metrics
cat('Test loss:', mmlp.score[[1]], '\n')
cat('Test accuracy:', mmlp.score[[2]], '\n')
```

## Q3

Fit a convolutional neural network and perform the same task in Q0.

You can refer to this example code. <https://tensorflow.rstudio.com/guide/keras/examples/mnist_cnn/>


```{r}

# Data Preparation

batch_size <- 128
num_classes <- 10
epochs <- 12

# Input image dimensions
img_rows <- 28
img_cols <- 28

# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)

# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255

cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')

# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)

# Define Model -----------------------------------------------------------

# Define model
mcnn <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                input_shape = input_shape) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = num_classes, activation = 'softmax')

# Compile model
mcnn %>% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)

# Train model
mcnn %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2
)


mcnn.scores <- mcnn %>% evaluate(
  x_test, y_test, verbose = 0
)

# Output metrics
cat('Test loss:', mcnn.scores[[1]], '\n')
cat('Test accuracy:', mcnn.scores[[2]], '\n')
```


## Q4

Summarize the prediction accuracy and runtime differences between these models.


```{r}
cbind(mlogit.score, mmlp.score, mcnn.scores)
```